{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7920 images belonging to 5 classes.\n",
      "Found 1980 images belonging to 5 classes.\n",
      "Found 100 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Directories\n",
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'\n",
    "\n",
    "# Data generator for training data\n",
    "#Data Aug....\n",
    "# Data generator for training data with validation split\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=40,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)  # 20% validation split\n",
    "\n",
    "# Separate generator for training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # Specify 'training' subset\n",
    "\n",
    "# Separate generator for validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')  # Specify 'validation' subset\n",
    "\n",
    "\n",
    "\n",
    "# Data generator for test data\n",
    "test_files = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n",
    "\n",
    "# Create a DataFrame with the filenames\n",
    "test_df = pd.DataFrame({\"Filename\": test_files})\n",
    "\n",
    "#Data Aug\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='Filename',\n",
    "    y_col=None,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 227, 227, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 114, 114, 64  9472        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 114, 114, 64  256        ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 114, 114, 64  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 57, 57, 64)   0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 57, 57, 64)   36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 57, 57, 64)  256         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 57, 57, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 57, 57, 64)   36928       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 57, 57, 64)   4160        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 57, 57, 64)  256         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 57, 57, 64)  256         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 57, 57, 64)   0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 57, 57, 64)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 57, 57, 64)   36928       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 57, 57, 64)  256         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 57, 57, 64)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 57, 57, 64)   36928       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57, 57, 64)  256         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 57, 57, 64)   0           ['batch_normalization_5[0][0]',  \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 57, 57, 64)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 57, 57, 64)   36928       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 57, 57, 64)  256         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 57, 57, 64)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 57, 57, 64)   36928       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 57, 57, 64)  256         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 57, 57, 64)   0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 57, 57, 64)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 29, 29, 128)  73856       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 29, 29, 128)  512        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 29, 29, 128)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 29, 29, 128)  147584      ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 29, 29, 128)  8320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 29, 29, 128)  512        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 29, 29, 128)  512        ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 29, 29, 128)  0           ['batch_normalization_10[0][0]', \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 29, 29, 128)  0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 29, 29, 128)  147584      ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 29, 29, 128)  512        ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 29, 29, 128)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 29, 29, 128)  147584      ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 29, 29, 128)  512        ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 29, 29, 128)  0           ['batch_normalization_12[0][0]', \n",
      "                                                                  'activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 29, 29, 128)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 29, 29, 128)  147584      ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 29, 29, 128)  512        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 29, 29, 128)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 29, 29, 128)  147584      ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 29, 29, 128)  512        ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 29, 29, 128)  0           ['batch_normalization_14[0][0]', \n",
      "                                                                  'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 29, 29, 128)  0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 29, 29, 128)  147584      ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 29, 29, 128)  512        ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 29, 29, 128)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 29, 29, 128)  147584      ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 29, 29, 128)  512        ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 29, 29, 128)  0           ['batch_normalization_16[0][0]', \n",
      "                                                                  'activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 29, 29, 128)  0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 15, 15, 256)  295168      ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 15, 15, 256)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 15, 15, 256)  33024       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 15, 15, 256)  0           ['batch_normalization_19[0][0]', \n",
      "                                                                  'batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 15, 15, 256)  0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 15, 15, 256)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 15, 15, 256)  0           ['batch_normalization_21[0][0]', \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 15, 15, 256)  0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 15, 15, 256)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 15, 15, 256)  0           ['batch_normalization_23[0][0]', \n",
      "                                                                  'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 15, 15, 256)  0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 15, 15, 256)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 15, 15, 256)  0           ['batch_normalization_25[0][0]', \n",
      "                                                                  'activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 15, 15, 256)  0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 15, 15, 256)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 15, 15, 256)  0           ['batch_normalization_27[0][0]', \n",
      "                                                                  'activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 15, 15, 256)  0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 15, 15, 256)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 15, 15, 256)  590080      ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 15, 15, 256)  1024       ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 15, 15, 256)  0           ['batch_normalization_29[0][0]', \n",
      "                                                                  'activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 15, 15, 256)  0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 512)    1180160     ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 512)    131584      ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 8, 8, 512)    0           ['batch_normalization_32[0][0]', \n",
      "                                                                  'batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 8, 8, 512)    0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 8, 8, 512)    0           ['batch_normalization_34[0][0]', \n",
      "                                                                  'activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 512)    0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 512)    0           ['batch_normalization_36[0][0]', \n",
      "                                                                  'activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 512)    0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 512)   0           ['activation_32[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            2565        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,317,189\n",
      "Trainable params: 21,300,037\n",
      "Non-trainable params: 17,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#####PRETRAINED MODEL\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Add, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def residual_block(x, filters, stride=1, use_projection=False):\n",
    "    shortcut = x\n",
    "    if use_projection:\n",
    "        shortcut = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    # First convolutional layer\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Add shortcut to the output\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def ResNet50(input_shape=(227, 227, 3), num_classes=5):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial convolutional layer\n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=2, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    x = residual_block(x, filters=64, use_projection=True)\n",
    "    x = residual_block(x, filters=64)\n",
    "    x = residual_block(x, filters=64)\n",
    "\n",
    "    x = residual_block(x, filters=128, stride=2, use_projection=True)\n",
    "    x = residual_block(x, filters=128)\n",
    "    x = residual_block(x, filters=128)\n",
    "    x = residual_block(x, filters=128)\n",
    "\n",
    "    x = residual_block(x, filters=256, stride=2, use_projection=True)\n",
    "    x = residual_block(x, filters=256)\n",
    "    x = residual_block(x, filters=256)\n",
    "    x = residual_block(x, filters=256)\n",
    "    x = residual_block(x, filters=256)\n",
    "    x = residual_block(x, filters=256)\n",
    "\n",
    "    x = residual_block(x, filters=512, stride=2, use_projection=True)\n",
    "    x = residual_block(x, filters=512)\n",
    "    x = residual_block(x, filters=512)\n",
    "\n",
    "    # Final layers\n",
    "    x = AveragePooling2D(pool_size=(7, 7))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Create ResNet50 model\n",
    "resnet50_model = ResNet50()\n",
    "\n",
    "# Display the model summary\n",
    "resnet50_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet50_model.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "248/248 [==============================] - 95s 342ms/step - loss: 1.3395 - accuracy: 0.4865 - val_loss: 2.7333 - val_accuracy: 0.2061\n",
      "Epoch 2/40\n",
      "248/248 [==============================] - 80s 324ms/step - loss: 1.0810 - accuracy: 0.5756 - val_loss: 1.6324 - val_accuracy: 0.3833\n",
      "Epoch 3/40\n",
      "248/248 [==============================] - 77s 311ms/step - loss: 1.0123 - accuracy: 0.6034 - val_loss: 1.3618 - val_accuracy: 0.4611\n",
      "Epoch 4/40\n",
      "248/248 [==============================] - 80s 320ms/step - loss: 0.9654 - accuracy: 0.6221 - val_loss: 1.5169 - val_accuracy: 0.4025\n",
      "Epoch 5/40\n",
      "248/248 [==============================] - 79s 319ms/step - loss: 0.9078 - accuracy: 0.6484 - val_loss: 1.2966 - val_accuracy: 0.4652\n",
      "Epoch 6/40\n",
      "248/248 [==============================] - 78s 314ms/step - loss: 0.9135 - accuracy: 0.6441 - val_loss: 1.4587 - val_accuracy: 0.4505\n",
      "Epoch 7/40\n",
      "248/248 [==============================] - 71s 285ms/step - loss: 0.8455 - accuracy: 0.6734 - val_loss: 1.1951 - val_accuracy: 0.5333\n",
      "Epoch 8/40\n",
      "248/248 [==============================] - 81s 326ms/step - loss: 0.8332 - accuracy: 0.6843 - val_loss: 1.1367 - val_accuracy: 0.5591\n",
      "Epoch 9/40\n",
      "248/248 [==============================] - 73s 295ms/step - loss: 0.8010 - accuracy: 0.6994 - val_loss: 1.2691 - val_accuracy: 0.5364\n",
      "Epoch 10/40\n",
      "248/248 [==============================] - 73s 295ms/step - loss: 0.7842 - accuracy: 0.7081 - val_loss: 1.2890 - val_accuracy: 0.5045\n",
      "Epoch 11/40\n",
      "248/248 [==============================] - 71s 287ms/step - loss: 0.8036 - accuracy: 0.6934 - val_loss: 0.9537 - val_accuracy: 0.6571\n",
      "Epoch 12/40\n",
      "248/248 [==============================] - 74s 298ms/step - loss: 0.7489 - accuracy: 0.7178 - val_loss: 0.9610 - val_accuracy: 0.6222\n",
      "Epoch 13/40\n",
      "248/248 [==============================] - 71s 284ms/step - loss: 0.7406 - accuracy: 0.7232 - val_loss: 1.8443 - val_accuracy: 0.4990\n",
      "Epoch 14/40\n",
      "248/248 [==============================] - 73s 293ms/step - loss: 0.7087 - accuracy: 0.7348 - val_loss: 1.1407 - val_accuracy: 0.6035\n",
      "Epoch 15/40\n",
      "248/248 [==============================] - 72s 291ms/step - loss: 0.6940 - accuracy: 0.7399 - val_loss: 1.6511 - val_accuracy: 0.4298\n",
      "Epoch 16/40\n",
      "248/248 [==============================] - 74s 299ms/step - loss: 0.6822 - accuracy: 0.7423 - val_loss: 0.9037 - val_accuracy: 0.6591\n",
      "Epoch 17/40\n",
      "248/248 [==============================] - 71s 285ms/step - loss: 0.6816 - accuracy: 0.7446 - val_loss: 1.8166 - val_accuracy: 0.4909\n",
      "Epoch 18/40\n",
      "248/248 [==============================] - 74s 296ms/step - loss: 0.6692 - accuracy: 0.7491 - val_loss: 0.8350 - val_accuracy: 0.7015\n",
      "Epoch 19/40\n",
      "248/248 [==============================] - 71s 287ms/step - loss: 0.6396 - accuracy: 0.7605 - val_loss: 1.0367 - val_accuracy: 0.6051\n",
      "Epoch 20/40\n",
      "248/248 [==============================] - 77s 309ms/step - loss: 0.6412 - accuracy: 0.7611 - val_loss: 1.1777 - val_accuracy: 0.6263\n",
      "Epoch 21/40\n",
      "248/248 [==============================] - 72s 288ms/step - loss: 0.6482 - accuracy: 0.7598 - val_loss: 10.3072 - val_accuracy: 0.5268\n",
      "Epoch 22/40\n",
      "248/248 [==============================] - 74s 298ms/step - loss: 0.6120 - accuracy: 0.7679 - val_loss: 1.8083 - val_accuracy: 0.6247\n",
      "Epoch 23/40\n",
      "248/248 [==============================] - 72s 291ms/step - loss: 0.6006 - accuracy: 0.7754 - val_loss: 1.1416 - val_accuracy: 0.6237\n",
      "Epoch 24/40\n",
      "248/248 [==============================] - 79s 317ms/step - loss: 0.5943 - accuracy: 0.7785 - val_loss: 2.1000 - val_accuracy: 0.4323\n",
      "Epoch 25/40\n",
      "248/248 [==============================] - 71s 284ms/step - loss: 0.5797 - accuracy: 0.7860 - val_loss: 0.9031 - val_accuracy: 0.6727\n",
      "Epoch 26/40\n",
      "248/248 [==============================] - 73s 295ms/step - loss: 0.5711 - accuracy: 0.7843 - val_loss: 1.1866 - val_accuracy: 0.5803\n",
      "Epoch 27/40\n",
      "248/248 [==============================] - 72s 291ms/step - loss: 0.5482 - accuracy: 0.7994 - val_loss: 1.1587 - val_accuracy: 0.6182\n",
      "Epoch 28/40\n",
      "248/248 [==============================] - 74s 296ms/step - loss: 0.5567 - accuracy: 0.8001 - val_loss: 1.0715 - val_accuracy: 0.6141\n",
      "Epoch 29/40\n",
      "248/248 [==============================] - 71s 286ms/step - loss: 0.5336 - accuracy: 0.8033 - val_loss: 0.9237 - val_accuracy: 0.6515\n",
      "Epoch 30/40\n",
      "248/248 [==============================] - 74s 298ms/step - loss: 0.5157 - accuracy: 0.8128 - val_loss: 0.7923 - val_accuracy: 0.7187\n",
      "Epoch 31/40\n",
      "248/248 [==============================] - 78s 312ms/step - loss: 0.5095 - accuracy: 0.8135 - val_loss: 1.2877 - val_accuracy: 0.5566\n",
      "Epoch 32/40\n",
      "248/248 [==============================] - 78s 314ms/step - loss: 0.4935 - accuracy: 0.8203 - val_loss: 1.0260 - val_accuracy: 0.6404\n",
      "Epoch 33/40\n",
      "248/248 [==============================] - 70s 283ms/step - loss: 0.4944 - accuracy: 0.8187 - val_loss: 1.0167 - val_accuracy: 0.6485\n",
      "Epoch 34/40\n",
      "248/248 [==============================] - 72s 288ms/step - loss: 0.4812 - accuracy: 0.8277 - val_loss: 1.0302 - val_accuracy: 0.6399\n",
      "Epoch 35/40\n",
      "248/248 [==============================] - 71s 287ms/step - loss: 0.4611 - accuracy: 0.8311 - val_loss: 0.9923 - val_accuracy: 0.6621\n",
      "Epoch 36/40\n",
      "248/248 [==============================] - 73s 292ms/step - loss: 0.4513 - accuracy: 0.8298 - val_loss: 1.2575 - val_accuracy: 0.6035\n",
      "Epoch 37/40\n",
      "248/248 [==============================] - 69s 279ms/step - loss: 0.4471 - accuracy: 0.8369 - val_loss: 0.9494 - val_accuracy: 0.6682\n",
      "Epoch 38/40\n",
      "248/248 [==============================] - 77s 309ms/step - loss: 0.4359 - accuracy: 0.8405 - val_loss: 0.8327 - val_accuracy: 0.7545\n",
      "Epoch 39/40\n",
      "248/248 [==============================] - 70s 282ms/step - loss: 0.4237 - accuracy: 0.8489 - val_loss: 1.1590 - val_accuracy: 0.6389\n",
      "Epoch 40/40\n",
      "248/248 [==============================] - 73s 294ms/step - loss: 0.4163 - accuracy: 0.8485 - val_loss: 0.8049 - val_accuracy: 0.7152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x233d4759f00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "               \n",
    "resnet50_model.fit(train_generator, \n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=40,\n",
    "    validation_data=validation_generator, \n",
    "    validation_steps=len(validation_generator),\n",
    "   ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####output\n",
    "Epoch 1/40\n",
    "248/248 [==============================] - 95s 342ms/step - loss: 1.3395 - accuracy: 0.4865 - val_loss: 2.7333 - val_accuracy: 0.2061\n",
    "Epoch 2/40\n",
    "248/248 [==============================] - 80s 324ms/step - loss: 1.0810 - accuracy: 0.5756 - val_loss: 1.6324 - val_accuracy: 0.3833\n",
    "Epoch 3/40\n",
    "248/248 [==============================] - 77s 311ms/step - loss: 1.0123 - accuracy: 0.6034 - val_loss: 1.3618 - val_accuracy: 0.4611\n",
    "Epoch 4/40\n",
    "248/248 [==============================] - 80s 320ms/step - loss: 0.9654 - accuracy: 0.6221 - val_loss: 1.5169 - val_accuracy: 0.4025\n",
    "Epoch 5/40\n",
    "248/248 [==============================] - 79s 319ms/step - loss: 0.9078 - accuracy: 0.6484 - val_loss: 1.2966 - val_accuracy: 0.4652\n",
    "Epoch 6/40\n",
    "248/248 [==============================] - 78s 314ms/step - loss: 0.9135 - accuracy: 0.6441 - val_loss: 1.4587 - val_accuracy: 0.4505\n",
    "Epoch 7/40\n",
    "248/248 [==============================] - 71s 285ms/step - loss: 0.8455 - accuracy: 0.6734 - val_loss: 1.1951 - val_accuracy: 0.5333\n",
    "Epoch 8/40\n",
    "248/248 [==============================] - 81s 326ms/step - loss: 0.8332 - accuracy: 0.6843 - val_loss: 1.1367 - val_accuracy: 0.5591\n",
    "Epoch 9/40\n",
    "248/248 [==============================] - 73s 295ms/step - loss: 0.8010 - accuracy: 0.6994 - val_loss: 1.2691 - val_accuracy: 0.5364\n",
    "Epoch 10/40\n",
    "248/248 [==============================] - 73s 295ms/step - loss: 0.7842 - accuracy: 0.7081 - val_loss: 1.2890 - val_accuracy: 0.5045\n",
    "Epoch 11/40\n",
    "248/248 [==============================] - 71s 287ms/step - loss: 0.8036 - accuracy: 0.6934 - val_loss: 0.9537 - val_accuracy: 0.6571\n",
    "Epoch 12/40\n",
    "248/248 [==============================] - 74s 298ms/step - loss: 0.7489 - accuracy: 0.7178 - val_loss: 0.9610 - val_accuracy: 0.6222\n",
    "Epoch 13/40\n",
    "248/248 [==============================] - 71s 284ms/step - loss: 0.7406 - accuracy: 0.7232 - val_loss: 1.8443 - val_accuracy: 0.4990\n",
    "Epoch 14/40\n",
    "248/248 [==============================] - 73s 293ms/step - loss: 0.7087 - accuracy: 0.7348 - val_loss: 1.1407 - val_accuracy: 0.6035\n",
    "Epoch 15/40\n",
    "248/248 [==============================] - 72s 291ms/step - loss: 0.6940 - accuracy: 0.7399 - val_loss: 1.6511 - val_accuracy: 0.4298\n",
    "Epoch 16/40\n",
    "248/248 [==============================] - 74s 299ms/step - loss: 0.6822 - accuracy: 0.7423 - val_loss: 0.9037 - val_accuracy: 0.6591\n",
    "Epoch 17/40\n",
    "248/248 [==============================] - 71s 285ms/step - loss: 0.6816 - accuracy: 0.7446 - val_loss: 1.8166 - val_accuracy: 0.4909\n",
    "Epoch 18/40\n",
    "248/248 [==============================] - 74s 296ms/step - loss: 0.6692 - accuracy: 0.7491 - val_loss: 0.8350 - val_accuracy: 0.7015\n",
    "Epoch 19/40\n",
    "248/248 [==============================] - 71s 287ms/step - loss: 0.6396 - accuracy: 0.7605 - val_loss: 1.0367 - val_accuracy: 0.6051\n",
    "Epoch 20/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.6412 - accuracy: 0.7611 - val_loss: 1.1777 - val_accuracy: 0.6263\n",
    "Epoch 21/40\n",
    "248/248 [==============================] - 72s 288ms/step - loss: 0.6482 - accuracy: 0.7598 - val_loss: 10.3072 - val_accuracy: 0.5268\n",
    "Epoch 22/40\n",
    "248/248 [==============================] - 74s 298ms/step - loss: 0.6120 - accuracy: 0.7679 - val_loss: 1.8083 - val_accuracy: 0.6247\n",
    "Epoch 23/40\n",
    "248/248 [==============================] - 72s 291ms/step - loss: 0.6006 - accuracy: 0.7754 - val_loss: 1.1416 - val_accuracy: 0.6237\n",
    "Epoch 24/40\n",
    "248/248 [==============================] - 79s 317ms/step - loss: 0.5943 - accuracy: 0.7785 - val_loss: 2.1000 - val_accuracy: 0.4323\n",
    "Epoch 25/40\n",
    "248/248 [==============================] - 71s 284ms/step - loss: 0.5797 - accuracy: 0.7860 - val_loss: 0.9031 - val_accuracy: 0.6727\n",
    "Epoch 26/40\n",
    "248/248 [==============================] - 73s 295ms/step - loss: 0.5711 - accuracy: 0.7843 - val_loss: 1.1866 - val_accuracy: 0.5803\n",
    "Epoch 27/40\n",
    "248/248 [==============================] - 72s 291ms/step - loss: 0.5482 - accuracy: 0.7994 - val_loss: 1.1587 - val_accuracy: 0.6182\n",
    "Epoch 28/40\n",
    "248/248 [==============================] - 74s 296ms/step - loss: 0.5567 - accuracy: 0.8001 - val_loss: 1.0715 - val_accuracy: 0.6141\n",
    "Epoch 29/40\n",
    "248/248 [==============================] - 71s 286ms/step - loss: 0.5336 - accuracy: 0.8033 - val_loss: 0.9237 - val_accuracy: 0.6515\n",
    "Epoch 30/40\n",
    "248/248 [==============================] - 74s 298ms/step - loss: 0.5157 - accuracy: 0.8128 - val_loss: 0.7923 - val_accuracy: 0.7187\n",
    "Epoch 31/40\n",
    "248/248 [==============================] - 78s 312ms/step - loss: 0.5095 - accuracy: 0.8135 - val_loss: 1.2877 - val_accuracy: 0.5566\n",
    "Epoch 32/40\n",
    "248/248 [==============================] - 78s 314ms/step - loss: 0.4935 - accuracy: 0.8203 - val_loss: 1.0260 - val_accuracy: 0.6404\n",
    "Epoch 33/40\n",
    "248/248 [==============================] - 70s 283ms/step - loss: 0.4944 - accuracy: 0.8187 - val_loss: 1.0167 - val_accuracy: 0.6485\n",
    "Epoch 34/40\n",
    "248/248 [==============================] - 72s 288ms/step - loss: 0.4812 - accuracy: 0.8277 - val_loss: 1.0302 - val_accuracy: 0.6399\n",
    "Epoch 35/40\n",
    "248/248 [==============================] - 71s 287ms/step - loss: 0.4611 - accuracy: 0.8311 - val_loss: 0.9923 - val_accuracy: 0.6621\n",
    "Epoch 36/40\n",
    "248/248 [==============================] - 73s 292ms/step - loss: 0.4513 - accuracy: 0.8298 - val_loss: 1.2575 - val_accuracy: 0.6035\n",
    "Epoch 37/40\n",
    "248/248 [==============================] - 69s 279ms/step - loss: 0.4471 - accuracy: 0.8369 - val_loss: 0.9494 - val_accuracy: 0.6682\n",
    "Epoch 38/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.4359 - accuracy: 0.8405 - val_loss: 0.8327 - val_accuracy: 0.7545\n",
    "Epoch 39/40\n",
    "248/248 [==============================] - 70s 282ms/step - loss: 0.4237 - accuracy: 0.8489 - val_loss: 1.1590 - val_accuracy: 0.6389\n",
    "Epoch 40/40\n",
    "248/248 [==============================] - 73s 294ms/step - loss: 0.4163 - accuracy: 0.8485 - val_loss: 0.8049 - val_accuracy: 0.7152\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "def residual_block(x, filters, stride=1, use_projection=False):\n",
    "    shortcut = x\n",
    "    if use_projection:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    # First convolutional layer\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add dropout for regularization\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Add shortcut to the output\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def ResNet50_modified(input_shape=(224, 224, 3), num_classes=5):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial convolutional layer\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "\n",
    "    # Reduced number of residual blocks for brevity\n",
    "    x = residual_block(x, filters=64, use_projection=True)\n",
    "    x = residual_block(x, filters=64)\n",
    "    x = residual_block(x, filters=128, stride=2, use_projection=True)\n",
    "    x = residual_block(x, filters=128)\n",
    "\n",
    "    # Fewer residual blocks for illustration purposes\n",
    "    x = residual_block(x, filters=256, stride=2, use_projection=True)\n",
    "    x = residual_block(x, filters=256)\n",
    "\n",
    "    x = layers.AveragePooling2D(pool_size=(7, 7))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Create the modified ResNet50 model\n",
    "modified_resnet50_model = ResNet50_modified()\n",
    "\n",
    "# Display the model summary\n",
    "modified_resnet50_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modified_resnet50_model.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "               \n",
    "modified_resnet50_model.fit(train_generator, \n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator, \n",
    "    validation_steps=len(validation_generator),\n",
    "   ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 38ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 1s 162ms/step\n",
      "[2 2 4 0 0 0 4 2 1 3 2 0 1 4 4 0 1 4 3 3 2 3 4 0 4 0 2 4 4 0 0 2 3 0 4 0 0\n",
      " 1 0 2 0 2 3 2 4 3 2 1 2 0 4 1 1 1 1 1 1 1 4 1 1 0 1 1 3 3 1 2 1 4 3 1 1 1\n",
      " 1 1 0 1 1 0 3 2 3 4 3 2 3 4 4 4 3 2 4 0 1 4 3 3 2 1]\n"
     ]
    }
   ],
   "source": [
    "accuracy = resnet50_model.evaluate(test_generator)\n",
    "# Make predictions on test data\n",
    "predictions = resnet50_model.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get only the filenames without the directory path\n",
    "test_generator.filenames = [os.path.basename(file) for file in test_generator.filenames]\n",
    "\n",
    "# Add 1 to each predicted class to shift the numbering\n",
    "predicted_classes_adjusted = predicted_classes + 1\n",
    "# Create a DataFrame to store filenames without paths and predicted classes\n",
    "results = pd.DataFrame({\"image_id\": test_generator.filenames, \"label\": predicted_classes_adjusted})\n",
    "\n",
    "# Save results to a CSV file\n",
    "results.to_csv('submission_resnet_GpU.csv', index=False)\n",
    "\n",
    "# Display predicted classes\n",
    "print(predicted_classes)\n",
    "\n",
    "# Save the trained model\n",
    "resnet50_model.save('trained_resnst50_model_GPU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 225, 225, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 110, 110, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 55, 55, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 193600)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               24780928  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,800,965\n",
      "Trainable params: 24,800,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 225, 225, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 112, 112, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 110, 110, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 55, 55, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 387200)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               99123456  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 99,200,389\n",
      "Trainable params: 99,200,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "248/248 [==============================] - 82s 310ms/step - loss: 1.5053 - accuracy: 0.4640 - val_loss: 1.2139 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "248/248 [==============================] - 67s 272ms/step - loss: 1.0787 - accuracy: 0.5525 - val_loss: 1.1716 - val_accuracy: 0.5071\n",
      "Epoch 3/50\n",
      "248/248 [==============================] - 70s 280ms/step - loss: 1.0288 - accuracy: 0.5822 - val_loss: 1.1248 - val_accuracy: 0.5333\n",
      "Epoch 4/50\n",
      "248/248 [==============================] - 73s 293ms/step - loss: 0.9605 - accuracy: 0.6112 - val_loss: 1.0640 - val_accuracy: 0.5732\n",
      "Epoch 5/50\n",
      "248/248 [==============================] - 70s 281ms/step - loss: 0.9416 - accuracy: 0.6246 - val_loss: 1.0836 - val_accuracy: 0.5611\n",
      "Epoch 6/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.9022 - accuracy: 0.6479 - val_loss: 1.0445 - val_accuracy: 0.5768\n",
      "Epoch 7/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.8698 - accuracy: 0.6571 - val_loss: 1.1210 - val_accuracy: 0.5545\n",
      "Epoch 8/50\n",
      "248/248 [==============================] - 70s 279ms/step - loss: 0.8512 - accuracy: 0.6617 - val_loss: 1.0071 - val_accuracy: 0.5975\n",
      "Epoch 9/50\n",
      "248/248 [==============================] - 74s 300ms/step - loss: 0.8298 - accuracy: 0.6723 - val_loss: 1.0199 - val_accuracy: 0.6000\n",
      "Epoch 10/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 0.8207 - accuracy: 0.6811 - val_loss: 1.0269 - val_accuracy: 0.6061\n",
      "Epoch 11/50\n",
      "248/248 [==============================] - 67s 269ms/step - loss: 0.7844 - accuracy: 0.6965 - val_loss: 1.0444 - val_accuracy: 0.5864\n",
      "Epoch 12/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.7872 - accuracy: 0.6890 - val_loss: 1.0339 - val_accuracy: 0.6192\n",
      "Epoch 13/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.7535 - accuracy: 0.7081 - val_loss: 0.9942 - val_accuracy: 0.6227\n",
      "Epoch 14/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.7464 - accuracy: 0.7052 - val_loss: 1.0942 - val_accuracy: 0.5934\n",
      "Epoch 15/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.7202 - accuracy: 0.7177 - val_loss: 1.0109 - val_accuracy: 0.6333\n",
      "Epoch 16/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.7007 - accuracy: 0.7290 - val_loss: 0.9910 - val_accuracy: 0.6268\n",
      "Epoch 17/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.6705 - accuracy: 0.7409 - val_loss: 1.0192 - val_accuracy: 0.6121\n",
      "Epoch 18/50\n",
      "248/248 [==============================] - 66s 267ms/step - loss: 0.6872 - accuracy: 0.7331 - val_loss: 1.0192 - val_accuracy: 0.6111\n",
      "Epoch 19/50\n",
      "248/248 [==============================] - 66s 267ms/step - loss: 0.6725 - accuracy: 0.7408 - val_loss: 1.0500 - val_accuracy: 0.6020\n",
      "Epoch 20/50\n",
      "248/248 [==============================] - 67s 268ms/step - loss: 0.6515 - accuracy: 0.7492 - val_loss: 1.0449 - val_accuracy: 0.5924\n",
      "Epoch 21/50\n",
      "248/248 [==============================] - 67s 267ms/step - loss: 0.6268 - accuracy: 0.7591 - val_loss: 1.0434 - val_accuracy: 0.6187\n",
      "Epoch 22/50\n",
      "248/248 [==============================] - 66s 267ms/step - loss: 0.6253 - accuracy: 0.7686 - val_loss: 1.0483 - val_accuracy: 0.6081\n",
      "Epoch 23/50\n",
      "248/248 [==============================] - 66s 266ms/step - loss: 0.6128 - accuracy: 0.7663 - val_loss: 1.0252 - val_accuracy: 0.6263\n",
      "Epoch 24/50\n",
      "248/248 [==============================] - 66s 266ms/step - loss: 0.5939 - accuracy: 0.7713 - val_loss: 1.0439 - val_accuracy: 0.6212\n",
      "Epoch 25/50\n",
      "248/248 [==============================] - 66s 267ms/step - loss: 0.5973 - accuracy: 0.7720 - val_loss: 0.9948 - val_accuracy: 0.6455\n",
      "Epoch 26/50\n",
      "248/248 [==============================] - 67s 267ms/step - loss: 0.5647 - accuracy: 0.7808 - val_loss: 1.0957 - val_accuracy: 0.6318\n",
      "Epoch 27/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.5578 - accuracy: 0.7896 - val_loss: 1.0578 - val_accuracy: 0.6293\n",
      "Epoch 28/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.5608 - accuracy: 0.7898 - val_loss: 1.0718 - val_accuracy: 0.6268\n",
      "Epoch 29/50\n",
      "248/248 [==============================] - 67s 269ms/step - loss: 0.5502 - accuracy: 0.7857 - val_loss: 1.0757 - val_accuracy: 0.6237\n",
      "Epoch 30/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.5324 - accuracy: 0.7977 - val_loss: 1.0389 - val_accuracy: 0.6384\n",
      "Epoch 31/50\n",
      "248/248 [==============================] - 67s 268ms/step - loss: 0.5232 - accuracy: 0.8037 - val_loss: 1.0900 - val_accuracy: 0.6167\n",
      "Epoch 32/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.5128 - accuracy: 0.8027 - val_loss: 1.0596 - val_accuracy: 0.6384\n",
      "Epoch 33/50\n",
      "248/248 [==============================] - 67s 268ms/step - loss: 0.4953 - accuracy: 0.8124 - val_loss: 1.1941 - val_accuracy: 0.6217\n",
      "Epoch 34/50\n",
      "248/248 [==============================] - 68s 271ms/step - loss: 0.4963 - accuracy: 0.8135 - val_loss: 1.1882 - val_accuracy: 0.6237\n",
      "Epoch 35/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.4691 - accuracy: 0.8234 - val_loss: 1.1160 - val_accuracy: 0.6278\n",
      "Epoch 36/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.4649 - accuracy: 0.8269 - val_loss: 1.1720 - val_accuracy: 0.6333\n",
      "Epoch 37/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.4610 - accuracy: 0.8211 - val_loss: 1.1585 - val_accuracy: 0.6167\n",
      "Epoch 38/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.4473 - accuracy: 0.8284 - val_loss: 1.2047 - val_accuracy: 0.6399\n",
      "Epoch 39/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.4531 - accuracy: 0.8308 - val_loss: 1.2725 - val_accuracy: 0.5980\n",
      "Epoch 40/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.4384 - accuracy: 0.8376 - val_loss: 1.1504 - val_accuracy: 0.6237\n",
      "Epoch 41/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.4226 - accuracy: 0.8439 - val_loss: 1.2110 - val_accuracy: 0.6409\n",
      "Epoch 42/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.4305 - accuracy: 0.8419 - val_loss: 1.1685 - val_accuracy: 0.6308\n",
      "Epoch 43/50\n",
      "248/248 [==============================] - 66s 267ms/step - loss: 0.4440 - accuracy: 0.8371 - val_loss: 1.1959 - val_accuracy: 0.6338\n",
      "Epoch 44/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.4241 - accuracy: 0.8378 - val_loss: 1.1794 - val_accuracy: 0.6490\n",
      "Epoch 45/50\n",
      "248/248 [==============================] - 67s 267ms/step - loss: 0.3967 - accuracy: 0.8523 - val_loss: 1.1958 - val_accuracy: 0.6449\n",
      "Epoch 46/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.3892 - accuracy: 0.8527 - val_loss: 1.2152 - val_accuracy: 0.6268\n",
      "Epoch 47/50\n",
      "248/248 [==============================] - 66s 267ms/step - loss: 0.3990 - accuracy: 0.8520 - val_loss: 1.2408 - val_accuracy: 0.6449\n",
      "Epoch 48/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.3922 - accuracy: 0.8568 - val_loss: 1.2542 - val_accuracy: 0.6222\n",
      "Epoch 49/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.3802 - accuracy: 0.8600 - val_loss: 1.2391 - val_accuracy: 0.6399\n",
      "Epoch 50/50\n",
      "248/248 [==============================] - 69s 276ms/step - loss: 0.3767 - accuracy: 0.8602 - val_loss: 1.2961 - val_accuracy: 0.6460\n",
      "Epoch 1/50\n",
      "248/248 [==============================] - 73s 285ms/step - loss: 1.6925 - accuracy: 0.4148 - val_loss: 1.2959 - val_accuracy: 0.4571\n",
      "Epoch 2/50\n",
      "248/248 [==============================] - 69s 279ms/step - loss: 1.1123 - accuracy: 0.5295 - val_loss: 1.2232 - val_accuracy: 0.4874\n",
      "Epoch 3/50\n",
      "248/248 [==============================] - 69s 278ms/step - loss: 1.0485 - accuracy: 0.5648 - val_loss: 1.1747 - val_accuracy: 0.5076\n",
      "Epoch 4/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 1.0103 - accuracy: 0.5835 - val_loss: 1.1392 - val_accuracy: 0.5293\n",
      "Epoch 5/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 0.9832 - accuracy: 0.5982 - val_loss: 1.1340 - val_accuracy: 0.5253\n",
      "Epoch 6/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.9510 - accuracy: 0.6122 - val_loss: 1.0915 - val_accuracy: 0.5399\n",
      "Epoch 7/50\n",
      "248/248 [==============================] - 71s 284ms/step - loss: 0.9391 - accuracy: 0.6169 - val_loss: 1.0892 - val_accuracy: 0.5414\n",
      "Epoch 8/50\n",
      "248/248 [==============================] - 69s 277ms/step - loss: 0.9263 - accuracy: 0.6256 - val_loss: 1.0596 - val_accuracy: 0.5636\n",
      "Epoch 9/50\n",
      "248/248 [==============================] - 69s 278ms/step - loss: 0.9160 - accuracy: 0.6304 - val_loss: 1.0767 - val_accuracy: 0.5384\n",
      "Epoch 10/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.8853 - accuracy: 0.6391 - val_loss: 1.0828 - val_accuracy: 0.5955\n",
      "Epoch 11/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.8754 - accuracy: 0.6538 - val_loss: 1.0366 - val_accuracy: 0.5783\n",
      "Epoch 12/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.8455 - accuracy: 0.6668 - val_loss: 1.0969 - val_accuracy: 0.5525\n",
      "Epoch 13/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.8403 - accuracy: 0.6711 - val_loss: 1.0317 - val_accuracy: 0.6081\n",
      "Epoch 14/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 0.8215 - accuracy: 0.6771 - val_loss: 1.0553 - val_accuracy: 0.5712\n",
      "Epoch 15/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.8075 - accuracy: 0.6852 - val_loss: 1.0341 - val_accuracy: 0.5783\n",
      "Epoch 16/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.7861 - accuracy: 0.6982 - val_loss: 1.0204 - val_accuracy: 0.6010\n",
      "Epoch 17/50\n",
      "248/248 [==============================] - 67s 270ms/step - loss: 0.7778 - accuracy: 0.6977 - val_loss: 1.0427 - val_accuracy: 0.5955\n",
      "Epoch 18/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.7753 - accuracy: 0.6999 - val_loss: 1.0495 - val_accuracy: 0.6040\n",
      "Epoch 19/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.7502 - accuracy: 0.7076 - val_loss: 1.0821 - val_accuracy: 0.5869\n",
      "Epoch 20/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.7332 - accuracy: 0.7121 - val_loss: 1.0002 - val_accuracy: 0.6010\n",
      "Epoch 21/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.7245 - accuracy: 0.7189 - val_loss: 1.0413 - val_accuracy: 0.5934\n",
      "Epoch 22/50\n",
      "248/248 [==============================] - 69s 276ms/step - loss: 0.7015 - accuracy: 0.7316 - val_loss: 0.9862 - val_accuracy: 0.6263\n",
      "Epoch 23/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 0.7037 - accuracy: 0.7311 - val_loss: 1.0053 - val_accuracy: 0.6318\n",
      "Epoch 24/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.6680 - accuracy: 0.7489 - val_loss: 1.0489 - val_accuracy: 0.6333\n",
      "Epoch 25/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.6520 - accuracy: 0.7489 - val_loss: 1.0403 - val_accuracy: 0.6308\n",
      "Epoch 26/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.6374 - accuracy: 0.7529 - val_loss: 1.0282 - val_accuracy: 0.6268\n",
      "Epoch 27/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.6251 - accuracy: 0.7585 - val_loss: 0.9829 - val_accuracy: 0.6460\n",
      "Epoch 28/50\n",
      "248/248 [==============================] - 67s 271ms/step - loss: 0.6132 - accuracy: 0.7663 - val_loss: 1.0164 - val_accuracy: 0.6338\n",
      "Epoch 29/50\n",
      "248/248 [==============================] - 69s 276ms/step - loss: 0.6012 - accuracy: 0.7649 - val_loss: 0.9949 - val_accuracy: 0.6444\n",
      "Epoch 30/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.6032 - accuracy: 0.7713 - val_loss: 1.0670 - val_accuracy: 0.6217\n",
      "Epoch 31/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.5896 - accuracy: 0.7766 - val_loss: 1.1077 - val_accuracy: 0.6162\n",
      "Epoch 32/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 0.5650 - accuracy: 0.7857 - val_loss: 1.0547 - val_accuracy: 0.6343\n",
      "Epoch 33/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 0.5666 - accuracy: 0.7822 - val_loss: 1.0428 - val_accuracy: 0.6348\n",
      "Epoch 34/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.5684 - accuracy: 0.7809 - val_loss: 1.0620 - val_accuracy: 0.6126\n",
      "Epoch 35/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.5549 - accuracy: 0.7905 - val_loss: 1.0403 - val_accuracy: 0.6520\n",
      "Epoch 36/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.5328 - accuracy: 0.7938 - val_loss: 1.0144 - val_accuracy: 0.6404\n",
      "Epoch 37/50\n",
      "248/248 [==============================] - 69s 277ms/step - loss: 0.5157 - accuracy: 0.8093 - val_loss: 1.1232 - val_accuracy: 0.6313\n",
      "Epoch 38/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.5139 - accuracy: 0.8057 - val_loss: 1.1110 - val_accuracy: 0.6313\n",
      "Epoch 39/50\n",
      "248/248 [==============================] - 71s 284ms/step - loss: 0.5040 - accuracy: 0.8104 - val_loss: 1.1681 - val_accuracy: 0.6313\n",
      "Epoch 40/50\n",
      "248/248 [==============================] - 70s 280ms/step - loss: 0.4912 - accuracy: 0.8136 - val_loss: 1.1156 - val_accuracy: 0.6323\n",
      "Epoch 41/50\n",
      "248/248 [==============================] - 77s 308ms/step - loss: 0.4811 - accuracy: 0.8210 - val_loss: 1.0455 - val_accuracy: 0.6535\n",
      "Epoch 42/50\n",
      "248/248 [==============================] - 75s 303ms/step - loss: 0.4659 - accuracy: 0.8275 - val_loss: 1.0881 - val_accuracy: 0.6429\n",
      "Epoch 43/50\n",
      "248/248 [==============================] - 69s 279ms/step - loss: 0.4636 - accuracy: 0.8301 - val_loss: 1.1448 - val_accuracy: 0.6253\n",
      "Epoch 44/50\n",
      "248/248 [==============================] - 73s 294ms/step - loss: 0.4518 - accuracy: 0.8314 - val_loss: 1.2463 - val_accuracy: 0.6247\n",
      "Epoch 45/50\n",
      "248/248 [==============================] - 69s 276ms/step - loss: 0.4508 - accuracy: 0.8333 - val_loss: 1.2164 - val_accuracy: 0.6389\n",
      "Epoch 46/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.4408 - accuracy: 0.8332 - val_loss: 1.2025 - val_accuracy: 0.6268\n",
      "Epoch 47/50\n",
      "248/248 [==============================] - 68s 274ms/step - loss: 0.4423 - accuracy: 0.8351 - val_loss: 1.2170 - val_accuracy: 0.6253\n",
      "Epoch 48/50\n",
      "248/248 [==============================] - 70s 281ms/step - loss: 0.4202 - accuracy: 0.8466 - val_loss: 1.1815 - val_accuracy: 0.6116\n",
      "Epoch 49/50\n",
      "248/248 [==============================] - 69s 276ms/step - loss: 0.4171 - accuracy: 0.8489 - val_loss: 1.1858 - val_accuracy: 0.6374\n",
      "Epoch 50/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 0.3937 - accuracy: 0.8578 - val_loss: 1.3639 - val_accuracy: 0.6263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First CNN model\n",
    "model1 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Second CNN model\n",
    "model2 = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display the model summary\n",
    "model1.summary()\n",
    "# Display the model summary\n",
    "model2.summary()\n",
    "# Compile both models\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train both models\n",
    "history1 = model1.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator)\n",
    "\n",
    "history2 = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator)\n",
    "\n",
    "# Save the trained model\n",
    "model1.save('saved_model1.h5')\n",
    "model2.save('saved_model2.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " conv2d (Conv2D)             (None, 225, 225, 32)      896       \n",
    "                                                                 \n",
    " max_pooling2d (MaxPooling2D  (None, 112, 112, 32)     0         \n",
    " )                                                               \n",
    "                                                                 \n",
    " conv2d_1 (Conv2D)           (None, 110, 110, 64)      18496     \n",
    "                                                                 \n",
    " max_pooling2d_1 (MaxPooling  (None, 55, 55, 64)       0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 193600)            0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 128)               24780928  \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 5)                 645       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 24,800,965\n",
    "Trainable params: 24,800,965\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " conv2d_2 (Conv2D)           (None, 225, 225, 64)      1792      \n",
    "                                                                 \n",
    " max_pooling2d_2 (MaxPooling  (None, 112, 112, 64)     0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_3 (Conv2D)           (None, 110, 110, 128)     73856     \n",
    "                                                                 \n",
    " max_pooling2d_3 (MaxPooling  (None, 55, 55, 128)      0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " flatten_1 (Flatten)         (None, 387200)            0         \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 256)               99123456  \n",
    "                                                                 \n",
    " dense_3 (Dense)             (None, 5)                 1285      \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 99,200,389\n",
    "Trainable params: 99,200,389\n",
    "Non-trainable params: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the trained model\n",
    "#model1 = load_model('saved_model2.h5')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model1.evaluate(test_generator)\n",
    ")\n",
    "predictions = model1.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get only the filenames without the directory path\n",
    "test_generator.filenames = [os.path.basename(file) for file in test_generator.filenames]\n",
    "\n",
    "# Add 1 to each predicted class to shift the numbering\n",
    "predicted_classes_adjusted = predicted_classes + 1\n",
    "# Create a DataFrame to store filenames without paths and predicted classes\n",
    "results = pd.DataFrame({\"image_id\": test_generator.filenames, \"label\": predicted_classes_adjusted})\n",
    "\n",
    "# Save results to a CSV file\n",
    "results.to_csv('submission_model1_GPU.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the trained model\n",
    "#model1 = load_model('saved_model2.h5')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model2.evaluate(test_generator)\n",
    "\n",
    "predictions = model2.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get only the filenames without the directory path\n",
    "test_generator.filenames = [os.path.basename(file) for file in test_generator.filenames]\n",
    "\n",
    "# Add 1 to each predicted class to shift the numbering\n",
    "predicted_classes_adjusted = predicted_classes + 1\n",
    "# Create a DataFrame to store filenames without paths and predicted classes\n",
    "results = pd.DataFrame({\"image_id\": test_generator.filenames, \"label\": predicted_classes_adjusted})\n",
    "\n",
    "# Save results to a CSV file\n",
    "results.to_csv('submission_model2_GPU.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############OUTPUT CNN 1\n",
    "Epoch 1/50\n",
    "248/248 [==============================] - 82s 310ms/step - loss: 1.5053 - accuracy: 0.4640 - val_loss: 1.2139 - val_accuracy: 0.5000\n",
    "Epoch 2/50\n",
    "248/248 [==============================] - 67s 272ms/step - loss: 1.0787 - accuracy: 0.5525 - val_loss: 1.1716 - val_accuracy: 0.5071\n",
    "Epoch 3/50\n",
    "248/248 [==============================] - 70s 280ms/step - loss: 1.0288 - accuracy: 0.5822 - val_loss: 1.1248 - val_accuracy: 0.5333\n",
    "Epoch 4/50\n",
    "248/248 [==============================] - 73s 293ms/step - loss: 0.9605 - accuracy: 0.6112 - val_loss: 1.0640 - val_accuracy: 0.5732\n",
    "Epoch 5/50\n",
    "248/248 [==============================] - 70s 281ms/step - loss: 0.9416 - accuracy: 0.6246 - val_loss: 1.0836 - val_accuracy: 0.5611\n",
    "Epoch 6/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.9022 - accuracy: 0.6479 - val_loss: 1.0445 - val_accuracy: 0.5768\n",
    "Epoch 7/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.8698 - accuracy: 0.6571 - val_loss: 1.1210 - val_accuracy: 0.5545\n",
    "Epoch 8/50\n",
    "248/248 [==============================] - 70s 279ms/step - loss: 0.8512 - accuracy: 0.6617 - val_loss: 1.0071 - val_accuracy: 0.5975\n",
    "Epoch 9/50\n",
    "248/248 [==============================] - 74s 300ms/step - loss: 0.8298 - accuracy: 0.6723 - val_loss: 1.0199 - val_accuracy: 0.6000\n",
    "Epoch 10/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 0.8207 - accuracy: 0.6811 - val_loss: 1.0269 - val_accuracy: 0.6061\n",
    "Epoch 11/50\n",
    "248/248 [==============================] - 67s 269ms/step - loss: 0.7844 - accuracy: 0.6965 - val_loss: 1.0444 - val_accuracy: 0.5864\n",
    "Epoch 12/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.7872 - accuracy: 0.6890 - val_loss: 1.0339 - val_accuracy: 0.6192\n",
    "Epoch 13/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.7535 - accuracy: 0.7081 - val_loss: 0.9942 - val_accuracy: 0.6227\n",
    "Epoch 14/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.7464 - accuracy: 0.7052 - val_loss: 1.0942 - val_accuracy: 0.5934\n",
    "Epoch 15/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.7202 - accuracy: 0.7177 - val_loss: 1.0109 - val_accuracy: 0.6333\n",
    "Epoch 16/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.7007 - accuracy: 0.7290 - val_loss: 0.9910 - val_accuracy: 0.6268\n",
    "Epoch 17/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.6705 - accuracy: 0.7409 - val_loss: 1.0192 - val_accuracy: 0.6121\n",
    "Epoch 18/50\n",
    "248/248 [==============================] - 66s 267ms/step - loss: 0.6872 - accuracy: 0.7331 - val_loss: 1.0192 - val_accuracy: 0.6111\n",
    "Epoch 19/50\n",
    "248/248 [==============================] - 66s 267ms/step - loss: 0.6725 - accuracy: 0.7408 - val_loss: 1.0500 - val_accuracy: 0.6020\n",
    "Epoch 20/50\n",
    "248/248 [==============================] - 67s 268ms/step - loss: 0.6515 - accuracy: 0.7492 - val_loss: 1.0449 - val_accuracy: 0.5924\n",
    "Epoch 21/50\n",
    "248/248 [==============================] - 67s 267ms/step - loss: 0.6268 - accuracy: 0.7591 - val_loss: 1.0434 - val_accuracy: 0.6187\n",
    "Epoch 22/50\n",
    "248/248 [==============================] - 66s 267ms/step - loss: 0.6253 - accuracy: 0.7686 - val_loss: 1.0483 - val_accuracy: 0.6081\n",
    "Epoch 23/50\n",
    "248/248 [==============================] - 66s 266ms/step - loss: 0.6128 - accuracy: 0.7663 - val_loss: 1.0252 - val_accuracy: 0.6263\n",
    "Epoch 24/50\n",
    "248/248 [==============================] - 66s 266ms/step - loss: 0.5939 - accuracy: 0.7713 - val_loss: 1.0439 - val_accuracy: 0.6212\n",
    "Epoch 25/50\n",
    "248/248 [==============================] - 66s 267ms/step - loss: 0.5973 - accuracy: 0.7720 - val_loss: 0.9948 - val_accuracy: 0.6455\n",
    "Epoch 26/50\n",
    "248/248 [==============================] - 67s 267ms/step - loss: 0.5647 - accuracy: 0.7808 - val_loss: 1.0957 - val_accuracy: 0.6318\n",
    "Epoch 27/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.5578 - accuracy: 0.7896 - val_loss: 1.0578 - val_accuracy: 0.6293\n",
    "Epoch 28/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.5608 - accuracy: 0.7898 - val_loss: 1.0718 - val_accuracy: 0.6268\n",
    "Epoch 29/50\n",
    "248/248 [==============================] - 67s 269ms/step - loss: 0.5502 - accuracy: 0.7857 - val_loss: 1.0757 - val_accuracy: 0.6237\n",
    "Epoch 30/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.5324 - accuracy: 0.7977 - val_loss: 1.0389 - val_accuracy: 0.6384\n",
    "Epoch 31/50\n",
    "248/248 [==============================] - 67s 268ms/step - loss: 0.5232 - accuracy: 0.8037 - val_loss: 1.0900 - val_accuracy: 0.6167\n",
    "Epoch 32/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.5128 - accuracy: 0.8027 - val_loss: 1.0596 - val_accuracy: 0.6384\n",
    "Epoch 33/50\n",
    "248/248 [==============================] - 67s 268ms/step - loss: 0.4953 - accuracy: 0.8124 - val_loss: 1.1941 - val_accuracy: 0.6217\n",
    "Epoch 34/50\n",
    "248/248 [==============================] - 68s 271ms/step - loss: 0.4963 - accuracy: 0.8135 - val_loss: 1.1882 - val_accuracy: 0.6237\n",
    "Epoch 35/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.4691 - accuracy: 0.8234 - val_loss: 1.1160 - val_accuracy: 0.6278\n",
    "Epoch 36/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.4649 - accuracy: 0.8269 - val_loss: 1.1720 - val_accuracy: 0.6333\n",
    "Epoch 37/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.4610 - accuracy: 0.8211 - val_loss: 1.1585 - val_accuracy: 0.6167\n",
    "Epoch 38/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.4473 - accuracy: 0.8284 - val_loss: 1.2047 - val_accuracy: 0.6399\n",
    "Epoch 39/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.4531 - accuracy: 0.8308 - val_loss: 1.2725 - val_accuracy: 0.5980\n",
    "Epoch 40/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.4384 - accuracy: 0.8376 - val_loss: 1.1504 - val_accuracy: 0.6237\n",
    "Epoch 41/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.4226 - accuracy: 0.8439 - val_loss: 1.2110 - val_accuracy: 0.6409\n",
    "Epoch 42/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.4305 - accuracy: 0.8419 - val_loss: 1.1685 - val_accuracy: 0.6308\n",
    "Epoch 43/50\n",
    "248/248 [==============================] - 66s 267ms/step - loss: 0.4440 - accuracy: 0.8371 - val_loss: 1.1959 - val_accuracy: 0.6338\n",
    "Epoch 44/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.4241 - accuracy: 0.8378 - val_loss: 1.1794 - val_accuracy: 0.6490\n",
    "Epoch 45/50\n",
    "248/248 [==============================] - 67s 267ms/step - loss: 0.3967 - accuracy: 0.8523 - val_loss: 1.1958 - val_accuracy: 0.6449\n",
    "Epoch 46/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.3892 - accuracy: 0.8527 - val_loss: 1.2152 - val_accuracy: 0.6268\n",
    "Epoch 47/50\n",
    "248/248 [==============================] - 66s 267ms/step - loss: 0.3990 - accuracy: 0.8520 - val_loss: 1.2408 - val_accuracy: 0.6449\n",
    "Epoch 48/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.3922 - accuracy: 0.8568 - val_loss: 1.2542 - val_accuracy: 0.6222\n",
    "Epoch 49/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.3802 - accuracy: 0.8600 - val_loss: 1.2391 - val_accuracy: 0.6399\n",
    "Epoch 50/50\n",
    "248/248 [==============================] - 69s 276ms/step - loss: 0.3767 - accuracy: 0.8602 - val_loss: 1.2961 - val_accuracy: 0.6460\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############OUTPUT CNN 2\n",
    "\n",
    "Epoch 1/50\n",
    "248/248 [==============================] - 73s 285ms/step - loss: 1.6925 - accuracy: 0.4148 - val_loss: 1.2959 - val_accuracy: 0.4571\n",
    "Epoch 2/50\n",
    "248/248 [==============================] - 69s 279ms/step - loss: 1.1123 - accuracy: 0.5295 - val_loss: 1.2232 - val_accuracy: 0.4874\n",
    "Epoch 3/50\n",
    "248/248 [==============================] - 69s 278ms/step - loss: 1.0485 - accuracy: 0.5648 - val_loss: 1.1747 - val_accuracy: 0.5076\n",
    "Epoch 4/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 1.0103 - accuracy: 0.5835 - val_loss: 1.1392 - val_accuracy: 0.5293\n",
    "Epoch 5/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 0.9832 - accuracy: 0.5982 - val_loss: 1.1340 - val_accuracy: 0.5253\n",
    "Epoch 6/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.9510 - accuracy: 0.6122 - val_loss: 1.0915 - val_accuracy: 0.5399\n",
    "Epoch 7/50\n",
    "248/248 [==============================] - 71s 284ms/step - loss: 0.9391 - accuracy: 0.6169 - val_loss: 1.0892 - val_accuracy: 0.5414\n",
    "Epoch 8/50\n",
    "248/248 [==============================] - 69s 277ms/step - loss: 0.9263 - accuracy: 0.6256 - val_loss: 1.0596 - val_accuracy: 0.5636\n",
    "Epoch 9/50\n",
    "248/248 [==============================] - 69s 278ms/step - loss: 0.9160 - accuracy: 0.6304 - val_loss: 1.0767 - val_accuracy: 0.5384\n",
    "Epoch 10/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.8853 - accuracy: 0.6391 - val_loss: 1.0828 - val_accuracy: 0.5955\n",
    "Epoch 11/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.8754 - accuracy: 0.6538 - val_loss: 1.0366 - val_accuracy: 0.5783\n",
    "Epoch 12/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.8455 - accuracy: 0.6668 - val_loss: 1.0969 - val_accuracy: 0.5525\n",
    "Epoch 13/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.8403 - accuracy: 0.6711 - val_loss: 1.0317 - val_accuracy: 0.6081\n",
    "Epoch 14/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 0.8215 - accuracy: 0.6771 - val_loss: 1.0553 - val_accuracy: 0.5712\n",
    "Epoch 15/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.8075 - accuracy: 0.6852 - val_loss: 1.0341 - val_accuracy: 0.5783\n",
    "Epoch 16/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.7861 - accuracy: 0.6982 - val_loss: 1.0204 - val_accuracy: 0.6010\n",
    "Epoch 17/50\n",
    "248/248 [==============================] - 67s 270ms/step - loss: 0.7778 - accuracy: 0.6977 - val_loss: 1.0427 - val_accuracy: 0.5955\n",
    "Epoch 18/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.7753 - accuracy: 0.6999 - val_loss: 1.0495 - val_accuracy: 0.6040\n",
    "Epoch 19/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.7502 - accuracy: 0.7076 - val_loss: 1.0821 - val_accuracy: 0.5869\n",
    "Epoch 20/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.7332 - accuracy: 0.7121 - val_loss: 1.0002 - val_accuracy: 0.6010\n",
    "Epoch 21/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.7245 - accuracy: 0.7189 - val_loss: 1.0413 - val_accuracy: 0.5934\n",
    "Epoch 22/50\n",
    "248/248 [==============================] - 69s 276ms/step - loss: 0.7015 - accuracy: 0.7316 - val_loss: 0.9862 - val_accuracy: 0.6263\n",
    "Epoch 23/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 0.7037 - accuracy: 0.7311 - val_loss: 1.0053 - val_accuracy: 0.6318\n",
    "Epoch 24/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.6680 - accuracy: 0.7489 - val_loss: 1.0489 - val_accuracy: 0.6333\n",
    "Epoch 25/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.6520 - accuracy: 0.7489 - val_loss: 1.0403 - val_accuracy: 0.6308\n",
    "Epoch 26/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.6374 - accuracy: 0.7529 - val_loss: 1.0282 - val_accuracy: 0.6268\n",
    "Epoch 27/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.6251 - accuracy: 0.7585 - val_loss: 0.9829 - val_accuracy: 0.6460\n",
    "Epoch 28/50\n",
    "248/248 [==============================] - 67s 271ms/step - loss: 0.6132 - accuracy: 0.7663 - val_loss: 1.0164 - val_accuracy: 0.6338\n",
    "Epoch 29/50\n",
    "248/248 [==============================] - 69s 276ms/step - loss: 0.6012 - accuracy: 0.7649 - val_loss: 0.9949 - val_accuracy: 0.6444\n",
    "Epoch 30/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.6032 - accuracy: 0.7713 - val_loss: 1.0670 - val_accuracy: 0.6217\n",
    "Epoch 31/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.5896 - accuracy: 0.7766 - val_loss: 1.1077 - val_accuracy: 0.6162\n",
    "Epoch 32/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 0.5650 - accuracy: 0.7857 - val_loss: 1.0547 - val_accuracy: 0.6343\n",
    "Epoch 33/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 0.5666 - accuracy: 0.7822 - val_loss: 1.0428 - val_accuracy: 0.6348\n",
    "Epoch 34/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.5684 - accuracy: 0.7809 - val_loss: 1.0620 - val_accuracy: 0.6126\n",
    "Epoch 35/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.5549 - accuracy: 0.7905 - val_loss: 1.0403 - val_accuracy: 0.6520\n",
    "Epoch 36/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.5328 - accuracy: 0.7938 - val_loss: 1.0144 - val_accuracy: 0.6404\n",
    "Epoch 37/50\n",
    "248/248 [==============================] - 69s 277ms/step - loss: 0.5157 - accuracy: 0.8093 - val_loss: 1.1232 - val_accuracy: 0.6313\n",
    "Epoch 38/50\n",
    "248/248 [==============================] - 68s 273ms/step - loss: 0.5139 - accuracy: 0.8057 - val_loss: 1.1110 - val_accuracy: 0.6313\n",
    "Epoch 39/50\n",
    "248/248 [==============================] - 71s 284ms/step - loss: 0.5040 - accuracy: 0.8104 - val_loss: 1.1681 - val_accuracy: 0.6313\n",
    "Epoch 40/50\n",
    "248/248 [==============================] - 70s 280ms/step - loss: 0.4912 - accuracy: 0.8136 - val_loss: 1.1156 - val_accuracy: 0.6323\n",
    "Epoch 41/50\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.4811 - accuracy: 0.8210 - val_loss: 1.0455 - val_accuracy: 0.6535\n",
    "Epoch 42/50\n",
    "248/248 [==============================] - 75s 303ms/step - loss: 0.4659 - accuracy: 0.8275 - val_loss: 1.0881 - val_accuracy: 0.6429\n",
    "Epoch 43/50\n",
    "248/248 [==============================] - 69s 279ms/step - loss: 0.4636 - accuracy: 0.8301 - val_loss: 1.1448 - val_accuracy: 0.6253\n",
    "Epoch 44/50\n",
    "248/248 [==============================] - 73s 294ms/step - loss: 0.4518 - accuracy: 0.8314 - val_loss: 1.2463 - val_accuracy: 0.6247\n",
    "Epoch 45/50\n",
    "248/248 [==============================] - 69s 276ms/step - loss: 0.4508 - accuracy: 0.8333 - val_loss: 1.2164 - val_accuracy: 0.6389\n",
    "Epoch 46/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.4408 - accuracy: 0.8332 - val_loss: 1.2025 - val_accuracy: 0.6268\n",
    "Epoch 47/50\n",
    "248/248 [==============================] - 68s 274ms/step - loss: 0.4423 - accuracy: 0.8351 - val_loss: 1.2170 - val_accuracy: 0.6253\n",
    "Epoch 48/50\n",
    "248/248 [==============================] - 70s 281ms/step - loss: 0.4202 - accuracy: 0.8466 - val_loss: 1.1815 - val_accuracy: 0.6116\n",
    "Epoch 49/50\n",
    "248/248 [==============================] - 69s 276ms/step - loss: 0.4171 - accuracy: 0.8489 - val_loss: 1.1858 - val_accuracy: 0.6374\n",
    "Epoch 50/50\n",
    "248/248 [==============================] - 68s 272ms/step - loss: 0.3937 - accuracy: 0.8578 - val_loss: 1.3639 - val_accuracy: 0.6263\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############TRANFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\badr0\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\badr0\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.Resizing(224, 224),\n",
    "#         layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "#         layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "#         layers.experimental.preprocessing.RandomZoom(\n",
    "#             height_factor=0.2, width_factor=0.2\n",
    "#         ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit, utils\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(224,224,3))\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "\n",
    "    pre_trained_model = vit.vit_l32(\n",
    "        image_size=224,\n",
    "        activation='relu',\n",
    "        pretrained=True,\n",
    "        include_top=True,\n",
    "        pretrained_top=True,\n",
    "        classes=5\n",
    "    )\n",
    "    pre_trained_model.trainable = False\n",
    "    features = pre_trained_model(augmented)\n",
    "    features = layers.Dense(512, activation=layers.PReLU())(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(256, activation=layers.PReLU())(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(128, activation=layers.PReLU())(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "\n",
    "    outputs = layers.Dense(5, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\badr0\\.conda\\envs\\py310\\lib\\site-packages\\vit_keras\\vit.py:139: UserWarning: Can only use pretrained_top with imagenet21k+imagenet2012 if classes = 1000. Setting manually.\n",
      "  warnings.warn(\n",
      "c:\\Users\\badr0\\.conda\\envs\\py310\\lib\\site-packages\\vit_keras\\utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 7, 7\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 224, 224, 3)      7         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " vit-l32 (Functional)        (None, 1000)              306535400 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               513024    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               33024     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307,213,684\n",
      "Trainable params: 678,277\n",
      "Non-trainable params: 306,535,407\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "248/248 [==============================] - 112s 353ms/step - loss: 0.8325 - accuracy: 0.6831 - val_loss: 0.6446 - val_accuracy: 0.7798\n",
      "Epoch 2/10\n",
      "248/248 [==============================] - 81s 328ms/step - loss: 0.5841 - accuracy: 0.7890 - val_loss: 0.5832 - val_accuracy: 0.7955\n",
      "Epoch 3/10\n",
      "248/248 [==============================] - 85s 342ms/step - loss: 0.5336 - accuracy: 0.8112 - val_loss: 0.5466 - val_accuracy: 0.8035\n",
      "Epoch 4/10\n",
      "248/248 [==============================] - 80s 321ms/step - loss: 0.4998 - accuracy: 0.8212 - val_loss: 0.5276 - val_accuracy: 0.8187\n",
      "Epoch 5/10\n",
      "248/248 [==============================] - 83s 336ms/step - loss: 0.4567 - accuracy: 0.8302 - val_loss: 0.4964 - val_accuracy: 0.8217\n",
      "Epoch 6/10\n",
      "248/248 [==============================] - 81s 326ms/step - loss: 0.4556 - accuracy: 0.8388 - val_loss: 0.5056 - val_accuracy: 0.8157\n",
      "Epoch 7/10\n",
      "248/248 [==============================] - 77s 309ms/step - loss: 0.4236 - accuracy: 0.8489 - val_loss: 0.4799 - val_accuracy: 0.8308\n",
      "Epoch 8/10\n",
      "248/248 [==============================] - 79s 317ms/step - loss: 0.4301 - accuracy: 0.8456 - val_loss: 0.4690 - val_accuracy: 0.8323\n",
      "Epoch 9/10\n",
      "248/248 [==============================] - 85s 341ms/step - loss: 0.4060 - accuracy: 0.8564 - val_loss: 0.4698 - val_accuracy: 0.8414\n",
      "Epoch 10/10\n",
      "248/248 [==============================] - 110s 443ms/step - loss: 0.4046 - accuracy: 0.8537 - val_loss: 0.4880 - val_accuracy: 0.8242\n",
      "Epoch 1/5\n",
      "248/248 [==============================] - 88s 350ms/step - loss: 0.3933 - accuracy: 0.8626 - val_loss: 0.4923 - val_accuracy: 0.8278\n",
      "Epoch 2/5\n",
      "248/248 [==============================] - 88s 349ms/step - loss: 0.3870 - accuracy: 0.8629 - val_loss: 0.4457 - val_accuracy: 0.8515\n",
      "Epoch 3/5\n",
      "248/248 [==============================] - 81s 320ms/step - loss: 0.3834 - accuracy: 0.8684 - val_loss: 0.4708 - val_accuracy: 0.8419\n",
      "Epoch 4/5\n",
      "248/248 [==============================] - 81s 324ms/step - loss: 0.3758 - accuracy: 0.8638 - val_loss: 0.4453 - val_accuracy: 0.8404\n",
      "Epoch 5/5\n",
      "248/248 [==============================] - 81s 324ms/step - loss: 0.3660 - accuracy: 0.8678 - val_loss: 0.4467 - val_accuracy: 0.8460\n",
      "4/4 [==============================] - 5s 41ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "\n",
    "    )\n",
    "    \n",
    "\n",
    "    model.trainable = True\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=10 // 2,\n",
    "\n",
    "        validation_data=validation_generator,\n",
    "\n",
    "    )\n",
    "\n",
    "    accuracy = model.evaluate(test_generator)\n",
    "    \n",
    "\n",
    "\n",
    "    #model.save(\"./vit_best_model.h5\")\n",
    "    return history\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "vit_classifier.summary()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER\n",
    "Epoch 1/40\n",
    "248/248 [==============================] - 104s 338ms/step - loss: 0.8277 - accuracy: 0.6889 - val_loss: 0.5984 - val_accuracy: 0.7874\n",
    "Epoch 2/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.5821 - accuracy: 0.7907 - val_loss: 0.5760 - val_accuracy: 0.7960\n",
    "Epoch 3/40\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.5331 - accuracy: 0.8080 - val_loss: 0.5695 - val_accuracy: 0.7869\n",
    "Epoch 4/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.4952 - accuracy: 0.8231 - val_loss: 0.5507 - val_accuracy: 0.8066\n",
    "Epoch 5/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.4642 - accuracy: 0.8309 - val_loss: 0.4866 - val_accuracy: 0.8293\n",
    "Epoch 6/40\n",
    "248/248 [==============================] - 76s 308ms/step - loss: 0.4453 - accuracy: 0.8402 - val_loss: 0.5154 - val_accuracy: 0.8152\n",
    "Epoch 7/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.4361 - accuracy: 0.8441 - val_loss: 0.5189 - val_accuracy: 0.8192\n",
    "Epoch 8/40\n",
    "248/248 [==============================] - 77s 311ms/step - loss: 0.4208 - accuracy: 0.8494 - val_loss: 0.4994 - val_accuracy: 0.8187\n",
    "Epoch 9/40\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.4105 - accuracy: 0.8556 - val_loss: 0.5434 - val_accuracy: 0.8172\n",
    "Epoch 10/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.3963 - accuracy: 0.8591 - val_loss: 0.4925 - val_accuracy: 0.8222\n",
    "Epoch 11/40\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.3941 - accuracy: 0.8629 - val_loss: 0.4630 - val_accuracy: 0.8288\n",
    "Epoch 12/40\n",
    "248/248 [==============================] - 76s 308ms/step - loss: 0.3822 - accuracy: 0.8662 - val_loss: 0.4490 - val_accuracy: 0.8419\n",
    "Epoch 13/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.3866 - accuracy: 0.8586 - val_loss: 0.4578 - val_accuracy: 0.8404\n",
    "Epoch 14/40\n",
    "248/248 [==============================] - 76s 305ms/step - loss: 0.3788 - accuracy: 0.8655 - val_loss: 0.4499 - val_accuracy: 0.8480\n",
    "Epoch 15/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.3578 - accuracy: 0.8766 - val_loss: 0.4207 - val_accuracy: 0.8444\n",
    "Epoch 16/40\n",
    "248/248 [==============================] - 76s 306ms/step - loss: 0.3483 - accuracy: 0.8749 - val_loss: 0.5112 - val_accuracy: 0.8298\n",
    "Epoch 17/40\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.3572 - accuracy: 0.8744 - val_loss: 0.4907 - val_accuracy: 0.8313\n",
    "Epoch 18/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.3533 - accuracy: 0.8777 - val_loss: 0.4622 - val_accuracy: 0.8348\n",
    "Epoch 19/40\n",
    "248/248 [==============================] - 78s 312ms/step - loss: 0.3546 - accuracy: 0.8759 - val_loss: 0.4387 - val_accuracy: 0.8475\n",
    "Epoch 20/40\n",
    "248/248 [==============================] - 80s 323ms/step - loss: 0.3412 - accuracy: 0.8756 - val_loss: 0.4191 - val_accuracy: 0.8455\n",
    "Epoch 21/40\n",
    "248/248 [==============================] - 78s 315ms/step - loss: 0.3386 - accuracy: 0.8793 - val_loss: 0.4500 - val_accuracy: 0.8429\n",
    "Epoch 22/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.3221 - accuracy: 0.8893 - val_loss: 0.4427 - val_accuracy: 0.8490\n",
    "Epoch 23/40\n",
    "248/248 [==============================] - 76s 306ms/step - loss: 0.3431 - accuracy: 0.8814 - val_loss: 0.4141 - val_accuracy: 0.8566\n",
    "Epoch 24/40\n",
    "248/248 [==============================] - 81s 325ms/step - loss: 0.3269 - accuracy: 0.8857 - val_loss: 0.4758 - val_accuracy: 0.8505\n",
    "Epoch 25/40\n",
    "248/248 [==============================] - 80s 323ms/step - loss: 0.3320 - accuracy: 0.8830 - val_loss: 0.4240 - val_accuracy: 0.8545\n",
    "Epoch 26/40\n",
    "248/248 [==============================] - 77s 311ms/step - loss: 0.3164 - accuracy: 0.8891 - val_loss: 0.4258 - val_accuracy: 0.8540\n",
    "Epoch 27/40\n",
    "248/248 [==============================] - 80s 322ms/step - loss: 0.3174 - accuracy: 0.8919 - val_loss: 0.4470 - val_accuracy: 0.8500\n",
    "Epoch 28/40\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.3044 - accuracy: 0.8929 - val_loss: 0.4503 - val_accuracy: 0.8535\n",
    "Epoch 29/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.3194 - accuracy: 0.8883 - val_loss: 0.4477 - val_accuracy: 0.8545\n",
    "Epoch 30/40\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.3119 - accuracy: 0.8909 - val_loss: 0.4205 - val_accuracy: 0.8495\n",
    "Epoch 31/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2947 - accuracy: 0.8961 - val_loss: 0.4215 - val_accuracy: 0.8566\n",
    "Epoch 32/40\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.2862 - accuracy: 0.8987 - val_loss: 0.4422 - val_accuracy: 0.8455\n",
    "Epoch 33/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.3128 - accuracy: 0.8934 - val_loss: 0.4183 - val_accuracy: 0.8581\n",
    "Epoch 34/40\n",
    "248/248 [==============================] - 76s 308ms/step - loss: 0.2953 - accuracy: 0.8952 - val_loss: 0.4067 - val_accuracy: 0.8667\n",
    "Epoch 35/40\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.2937 - accuracy: 0.8981 - val_loss: 0.4417 - val_accuracy: 0.8525\n",
    "Epoch 36/40\n",
    "248/248 [==============================] - 78s 313ms/step - loss: 0.2927 - accuracy: 0.8979 - val_loss: 0.4562 - val_accuracy: 0.8439\n",
    "Epoch 37/40\n",
    "248/248 [==============================] - 79s 317ms/step - loss: 0.2903 - accuracy: 0.9000 - val_loss: 0.4307 - val_accuracy: 0.8545\n",
    "Epoch 38/40\n",
    "248/248 [==============================] - 81s 326ms/step - loss: 0.2911 - accuracy: 0.8966 - val_loss: 0.4528 - val_accuracy: 0.8530\n",
    "Epoch 39/40\n",
    "248/248 [==============================] - 78s 313ms/step - loss: 0.2829 - accuracy: 0.9024 - val_loss: 0.4411 - val_accuracy: 0.8515\n",
    "Epoch 40/40\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2863 - accuracy: 0.8958 - val_loss: 0.5121 - val_accuracy: 0.8399\n",
    "Epoch 1/20\n",
    "248/248 [==============================] - 77s 311ms/step - loss: 0.2772 - accuracy: 0.9028 - val_loss: 0.4174 - val_accuracy: 0.8596\n",
    "Epoch 2/20\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2745 - accuracy: 0.9078 - val_loss: 0.4680 - val_accuracy: 0.8343\n",
    "Epoch 3/20\n",
    "248/248 [==============================] - 77s 311ms/step - loss: 0.2734 - accuracy: 0.9042 - val_loss: 0.4889 - val_accuracy: 0.8444\n",
    "Epoch 4/20\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.2788 - accuracy: 0.9057 - val_loss: 0.4051 - val_accuracy: 0.8626\n",
    "Epoch 5/20\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.2772 - accuracy: 0.9056 - val_loss: 0.4664 - val_accuracy: 0.8530\n",
    "Epoch 6/20\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2738 - accuracy: 0.9053 - val_loss: 0.5409 - val_accuracy: 0.8258\n",
    "Epoch 7/20\n",
    "248/248 [==============================] - 77s 311ms/step - loss: 0.2612 - accuracy: 0.9082 - val_loss: 0.4639 - val_accuracy: 0.8591\n",
    "Epoch 8/20\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2690 - accuracy: 0.9038 - val_loss: 0.4245 - val_accuracy: 0.8662\n",
    "Epoch 9/20\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2611 - accuracy: 0.9087 - val_loss: 0.5023 - val_accuracy: 0.8419\n",
    "Epoch 10/20\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2614 - accuracy: 0.9116 - val_loss: 0.4963 - val_accuracy: 0.8384\n",
    "Epoch 11/20\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.2718 - accuracy: 0.9043 - val_loss: 0.4471 - val_accuracy: 0.8505\n",
    "Epoch 12/20\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.2613 - accuracy: 0.9095 - val_loss: 0.4336 - val_accuracy: 0.8561\n",
    "Epoch 13/20\n",
    "248/248 [==============================] - 77s 309ms/step - loss: 0.2543 - accuracy: 0.9111 - val_loss: 0.4350 - val_accuracy: 0.8455\n",
    "Epoch 14/20\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.2552 - accuracy: 0.9086 - val_loss: 0.4328 - val_accuracy: 0.8556\n",
    "Epoch 15/20\n",
    "248/248 [==============================] - 80s 321ms/step - loss: 0.2379 - accuracy: 0.9165 - val_loss: 0.4548 - val_accuracy: 0.8505\n",
    "Epoch 16/20\n",
    "248/248 [==============================] - 79s 319ms/step - loss: 0.2585 - accuracy: 0.9109 - val_loss: 0.4483 - val_accuracy: 0.8556\n",
    "Epoch 17/20\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.2436 - accuracy: 0.9162 - val_loss: 0.4635 - val_accuracy: 0.8561\n",
    "Epoch 18/20\n",
    "248/248 [==============================] - 77s 308ms/step - loss: 0.2471 - accuracy: 0.9155 - val_loss: 0.4757 - val_accuracy: 0.8485\n",
    "Epoch 19/20\n",
    "248/248 [==============================] - 77s 310ms/step - loss: 0.2563 - accuracy: 0.9129 - val_loss: 0.4316 - val_accuracy: 0.8657\n",
    "Epoch 20/20\n",
    "248/248 [==============================] - 79s 317ms/step - loss: 0.2434 - accuracy: 0.9139 - val_loss: 0.4313 - val_accuracy: 0.8566\n",
    "4/4 [==============================] - 4s 33ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "4/4 [==============================] - 5s 168ms/step\n",
      "[2 2 4 0 2 0 4 2 1 3 1 4 0 4 0 0 0 4 3 3 2 3 2 0 4 0 2 4 4 1 0 2 3 0 4 4 0\n",
      " 3 0 2 0 2 0 3 4 3 2 2 2 0 4 1 3 1 1 1 1 1 0 1 1 0 1 1 3 0 1 2 1 4 3 1 1 1\n",
      " 1 1 2 1 1 0 3 2 3 0 3 2 3 4 4 2 3 2 4 3 3 4 3 3 2 0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import save_model\n",
    "accuracy = vit_classifier.evaluate(test_generator)\n",
    "# Make predictions on test data\n",
    "predictions = vit_classifier.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get only the filenames without the directory path\n",
    "test_generator.filenames = [os.path.basename(file) for file in test_generator.filenames]\n",
    "\n",
    "# Add 1 to each predicted class to shift the numbering\n",
    "predicted_classes_adjusted = predicted_classes + 1\n",
    "# Create a DataFrame to store filenames without paths and predicted classes\n",
    "results = pd.DataFrame({\"image_id\": test_generator.filenames, \"label\": predicted_classes_adjusted})\n",
    "\n",
    "# Save results to a CSV file\n",
    "results.to_csv('submission_rvit.csv', index=False)\n",
    "\n",
    "# Display predicted classes\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######ALEX NET\n",
    "\n",
    "import keras\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(224,224,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024,activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024,activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(5,activation='softmax')  \n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 54, 54, 128)       46592     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 54, 54, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 27, 27, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 27, 27, 256)       819456    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 9, 9, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 9, 9, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 9, 9, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 9, 9, 256)         65792     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 9, 9, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 9, 9, 256)         65792     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 9, 9, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 4, 4, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1024)              4195328   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,842,373\n",
      "Trainable params: 6,840,069\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']    \n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "248/248 [==============================] - 68s 271ms/step - loss: 1.8903 - accuracy: 0.4037 - val_loss: 1.6548 - val_accuracy: 0.2753\n",
      "Epoch 2/50\n",
      "248/248 [==============================] - 66s 266ms/step - loss: 1.2663 - accuracy: 0.4879 - val_loss: 2.0899 - val_accuracy: 0.2702\n",
      "Epoch 3/50\n",
      "248/248 [==============================] - 68s 272ms/step - loss: 1.1476 - accuracy: 0.5307 - val_loss: 1.4332 - val_accuracy: 0.3747\n",
      "Epoch 4/50\n",
      "248/248 [==============================] - 67s 269ms/step - loss: 1.1153 - accuracy: 0.5525 - val_loss: 1.2825 - val_accuracy: 0.4364\n",
      "Epoch 5/50\n",
      "248/248 [==============================] - 69s 277ms/step - loss: 1.0653 - accuracy: 0.5794 - val_loss: 1.2759 - val_accuracy: 0.4601\n",
      "Epoch 6/50\n",
      "248/248 [==============================] - 70s 282ms/step - loss: 1.0196 - accuracy: 0.6003 - val_loss: 1.3848 - val_accuracy: 0.4520\n",
      "Epoch 7/50\n",
      "248/248 [==============================] - 76s 307ms/step - loss: 1.0055 - accuracy: 0.6130 - val_loss: 1.1646 - val_accuracy: 0.5258\n",
      "Epoch 8/50\n",
      "248/248 [==============================] - 67s 269ms/step - loss: 0.9902 - accuracy: 0.6158 - val_loss: 1.0996 - val_accuracy: 0.5545\n",
      "Epoch 9/50\n",
      "248/248 [==============================] - 69s 277ms/step - loss: 0.9713 - accuracy: 0.6360 - val_loss: 1.2617 - val_accuracy: 0.5086\n",
      "Epoch 10/50\n",
      "248/248 [==============================] - 73s 295ms/step - loss: 0.9392 - accuracy: 0.6424 - val_loss: 1.1390 - val_accuracy: 0.5763\n",
      "Epoch 11/50\n",
      "248/248 [==============================] - 70s 279ms/step - loss: 0.9220 - accuracy: 0.6516 - val_loss: 1.0941 - val_accuracy: 0.5591\n",
      "Epoch 12/50\n",
      "248/248 [==============================] - 72s 289ms/step - loss: 0.8930 - accuracy: 0.6630 - val_loss: 1.0609 - val_accuracy: 0.5864\n",
      "Epoch 13/50\n",
      "248/248 [==============================] - 72s 288ms/step - loss: 0.8687 - accuracy: 0.6764 - val_loss: 1.1963 - val_accuracy: 0.4965\n",
      "Epoch 14/50\n",
      "248/248 [==============================] - 69s 277ms/step - loss: 0.8469 - accuracy: 0.6821 - val_loss: 1.0346 - val_accuracy: 0.5970\n",
      "Epoch 15/50\n",
      "248/248 [==============================] - 72s 291ms/step - loss: 0.8299 - accuracy: 0.6923 - val_loss: 0.9871 - val_accuracy: 0.6167\n",
      "Epoch 16/50\n",
      "248/248 [==============================] - 68s 273ms/step - loss: 0.7956 - accuracy: 0.6996 - val_loss: 0.9549 - val_accuracy: 0.6465\n",
      "Epoch 17/50\n",
      "248/248 [==============================] - 69s 277ms/step - loss: 0.7820 - accuracy: 0.7152 - val_loss: 1.0316 - val_accuracy: 0.6101\n",
      "Epoch 18/50\n",
      "248/248 [==============================] - 69s 276ms/step - loss: 0.7646 - accuracy: 0.7160 - val_loss: 0.9308 - val_accuracy: 0.6460\n",
      "Epoch 19/50\n",
      "248/248 [==============================] - 67s 267ms/step - loss: 0.7525 - accuracy: 0.7245 - val_loss: 0.9162 - val_accuracy: 0.6429\n",
      "Epoch 20/50\n",
      "248/248 [==============================] - 66s 267ms/step - loss: 0.7295 - accuracy: 0.7362 - val_loss: 0.9400 - val_accuracy: 0.6571\n",
      "Epoch 21/50\n",
      "248/248 [==============================] - 85s 342ms/step - loss: 0.7235 - accuracy: 0.7384 - val_loss: 1.0224 - val_accuracy: 0.6157\n",
      "Epoch 22/50\n",
      "248/248 [==============================] - 158s 638ms/step - loss: 0.7055 - accuracy: 0.7441 - val_loss: 0.9247 - val_accuracy: 0.6571\n",
      "Epoch 23/50\n",
      "248/248 [==============================] - 226s 900ms/step - loss: 0.6734 - accuracy: 0.7484 - val_loss: 0.9695 - val_accuracy: 0.6505\n",
      "Epoch 24/50\n",
      "248/248 [==============================] - 120s 472ms/step - loss: 0.6795 - accuracy: 0.7561 - val_loss: 0.9473 - val_accuracy: 0.6490\n",
      "Epoch 25/50\n",
      "248/248 [==============================] - 70s 277ms/step - loss: 0.6619 - accuracy: 0.7586 - val_loss: 1.2839 - val_accuracy: 0.5722\n",
      "Epoch 26/50\n",
      "248/248 [==============================] - 69s 271ms/step - loss: 0.6461 - accuracy: 0.7688 - val_loss: 0.9957 - val_accuracy: 0.6424\n",
      "Epoch 27/50\n",
      "248/248 [==============================] - 71s 279ms/step - loss: 0.6483 - accuracy: 0.7585 - val_loss: 0.9987 - val_accuracy: 0.6182\n",
      "Epoch 28/50\n",
      "248/248 [==============================] - 69s 271ms/step - loss: 0.6224 - accuracy: 0.7761 - val_loss: 1.1136 - val_accuracy: 0.6005\n",
      "Epoch 29/50\n",
      "248/248 [==============================] - 67s 265ms/step - loss: 0.6004 - accuracy: 0.7845 - val_loss: 1.3182 - val_accuracy: 0.5424\n",
      "Epoch 30/50\n",
      "248/248 [==============================] - 67s 267ms/step - loss: 0.5958 - accuracy: 0.7773 - val_loss: 0.8463 - val_accuracy: 0.6965\n",
      "Epoch 31/50\n",
      "248/248 [==============================] - 68s 270ms/step - loss: 0.5796 - accuracy: 0.7884 - val_loss: 1.1723 - val_accuracy: 0.6313\n",
      "Epoch 32/50\n",
      "248/248 [==============================] - 69s 272ms/step - loss: 0.5667 - accuracy: 0.7948 - val_loss: 1.5287 - val_accuracy: 0.5444\n",
      "Epoch 33/50\n",
      "248/248 [==============================] - 69s 271ms/step - loss: 0.5527 - accuracy: 0.7986 - val_loss: 2.2422 - val_accuracy: 0.4258\n",
      "Epoch 34/50\n",
      "248/248 [==============================] - 70s 274ms/step - loss: 0.5402 - accuracy: 0.8013 - val_loss: 1.1519 - val_accuracy: 0.6035\n",
      "Epoch 35/50\n",
      "248/248 [==============================] - 69s 272ms/step - loss: 0.5404 - accuracy: 0.8038 - val_loss: 1.2811 - val_accuracy: 0.5707\n",
      "Epoch 36/50\n",
      "248/248 [==============================] - 74s 292ms/step - loss: 0.5315 - accuracy: 0.8093 - val_loss: 0.9980 - val_accuracy: 0.6384\n",
      "Epoch 37/50\n",
      "248/248 [==============================] - 71s 280ms/step - loss: 0.5168 - accuracy: 0.8129 - val_loss: 1.0805 - val_accuracy: 0.6646\n",
      "Epoch 38/50\n",
      "248/248 [==============================] - 72s 284ms/step - loss: 0.4945 - accuracy: 0.8196 - val_loss: 0.9460 - val_accuracy: 0.6960\n",
      "Epoch 39/50\n",
      "248/248 [==============================] - 73s 291ms/step - loss: 0.4874 - accuracy: 0.8240 - val_loss: 1.0394 - val_accuracy: 0.6571\n",
      "Epoch 40/50\n",
      "248/248 [==============================] - 70s 279ms/step - loss: 0.4777 - accuracy: 0.8304 - val_loss: 0.8866 - val_accuracy: 0.7010\n",
      "Epoch 41/50\n",
      "248/248 [==============================] - 74s 293ms/step - loss: 0.4660 - accuracy: 0.8336 - val_loss: 0.9157 - val_accuracy: 0.7313\n",
      "Epoch 42/50\n",
      "248/248 [==============================] - 73s 290ms/step - loss: 0.4612 - accuracy: 0.8337 - val_loss: 0.9323 - val_accuracy: 0.6919\n",
      "Epoch 43/50\n",
      "248/248 [==============================] - 68s 266ms/step - loss: 0.4624 - accuracy: 0.8328 - val_loss: 1.1074 - val_accuracy: 0.6465\n",
      "Epoch 44/50\n",
      "248/248 [==============================] - 68s 270ms/step - loss: 0.4525 - accuracy: 0.8352 - val_loss: 0.9140 - val_accuracy: 0.6783\n",
      "Epoch 45/50\n",
      "248/248 [==============================] - 67s 265ms/step - loss: 0.4222 - accuracy: 0.8475 - val_loss: 1.0781 - val_accuracy: 0.7030\n",
      "Epoch 46/50\n",
      "248/248 [==============================] - 67s 265ms/step - loss: 0.4259 - accuracy: 0.8443 - val_loss: 1.4111 - val_accuracy: 0.6081\n",
      "Epoch 47/50\n",
      "248/248 [==============================] - 67s 267ms/step - loss: 0.4195 - accuracy: 0.8501 - val_loss: 0.9660 - val_accuracy: 0.6955\n",
      "Epoch 48/50\n",
      "248/248 [==============================] - 67s 264ms/step - loss: 0.4069 - accuracy: 0.8533 - val_loss: 1.0502 - val_accuracy: 0.6828\n",
      "Epoch 49/50\n",
      "248/248 [==============================] - 69s 273ms/step - loss: 0.4001 - accuracy: 0.8585 - val_loss: 0.8679 - val_accuracy: 0.7359\n",
      "Epoch 50/50\n",
      "248/248 [==============================] - 69s 271ms/step - loss: 0.3906 - accuracy: 0.8585 - val_loss: 0.9986 - val_accuracy: 0.6939\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
